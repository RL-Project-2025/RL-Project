{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2551241",
   "metadata": {},
   "source": [
    "# Benchmarks for Microgrid environment\n",
    "\n",
    "The notebook provides testing and comparison between deterministic policies and a pretrained RL agent strategies among the `MicroGridEnv` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756ffdd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:30.767740Z",
     "start_time": "2025-05-15T14:32:30.633749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/giovannidispoto/Desktop/PhD/gym4ReaL_github/gym4ReaL\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f82ead8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:30.839186Z",
     "start_time": "2025-05-15T14:32:30.769966Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "\n",
    "from gym4real.envs.microgrid.utils import parameter_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f59cf43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:33.352617Z",
     "start_time": "2025-05-15T14:32:33.326778Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plot_colors = sns.color_palette()\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "alg_color = OrderedDict({\n",
    "    'random': plot_colors[0],\n",
    "    'only_market': plot_colors[1],\n",
    "    'battery_first': plot_colors[2],\n",
    "    '50-50': plot_colors[3],\n",
    "    'ppo': plot_colors[4]\n",
    "})\n",
    "\n",
    "alg_markers = OrderedDict({\n",
    "    'random': '.',\n",
    "    'only_market': 'o',\n",
    "    'battery_first': 'v',\n",
    "    '50-50': 'P',\n",
    "    'ppo': '*',\n",
    "})\n",
    "\n",
    "alg_labels = {\n",
    "    'random': 'Random',\n",
    "    'only_market': 'OM',\n",
    "    'battery_first': 'BF',\n",
    "    '50-50': '50-50',\n",
    "    'ppo': 'PPO',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed53fe83-9efd-422a-bd60-41b7633afe59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:36.094576Z",
     "start_time": "2025-05-15T14:32:36.055973Z"
    }
   },
   "outputs": [],
   "source": [
    "# IF RESULTS ARE ALREADY AVAILABLE, GO TO LAST CELLS TO LOAD THEM FROM JSON FILE AND DIRECTLY PLOT THEM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733076f5",
   "metadata": {},
   "source": [
    "## Deterministic policies\n",
    "Here we can evaluate different rule-based and deterministic policies.\n",
    "Hereafter we will test:\n",
    "1. random action policy\n",
    "2. market-only policy\n",
    "3. battery-first policy\n",
    "5. 50/50 policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa19f7f-ab98-4c23-8c84-446891c3ca61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:39.702221Z",
     "start_time": "2025-05-15T14:32:39.540796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannidispoto/miniforge3/envs/gym4real/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001B[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001B[0m\n",
      "  gym.logger.warn(\n",
      "/Users/giovannidispoto/miniforge3/envs/gym4real/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001B[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001B[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "params = parameter_generator(world_options='gym4real/envs/microgrid/world_test.yaml')\n",
    "env = gym.make(id=\"gym4real/microgrid-v0\", **{'settings':params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b1615b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:32:43.009554Z",
     "start_time": "2025-05-15T14:32:42.989362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test profiles belonging to the test set\n",
    "test_profiles = [i for i in range(370, 398)]\n",
    "rewards = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c54d0",
   "metadata": {},
   "source": [
    "<h3> Random Policy: </h3>\n",
    "The action is chosen randomly at each decision step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3e6651",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:34:20.944121Z",
     "start_time": "2025-05-15T14:32:44.725640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:36<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "alg = 'random'\n",
    "rewards[alg] = {}\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    obs, info = env.reset(options={'eval_profile': str(profile)})\n",
    "    done = False\n",
    "    rewards[alg][profile] = {}\n",
    "    rewards[alg][profile]['pure'] = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Randomly select an action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  \n",
    "        done = terminated or truncated\n",
    "        rewards[alg][profile]['pure'].append(list(info['pure_rewards'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebbc2c",
   "metadata": {},
   "source": [
    "<h3> Only Market Policy: </h3>\n",
    "The action chosen is always 0, meaning that the battery is never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab672d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:35:20.586580Z",
     "start_time": "2025-05-15T14:34:20.945690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:59<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "alg = 'only_market'\n",
    "rewards[alg] = {}\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    obs, info = env.reset(options={'eval_profile': str(profile)})\n",
    "    done = False\n",
    "    rewards[alg][profile] = {}\n",
    "    rewards[alg][profile]['pure'] = []\n",
    "\n",
    "    while not done:\n",
    "        action = np.array([0]) # Only trading with market\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  \n",
    "        done = terminated or truncated\n",
    "        rewards[alg][profile]['pure'].append(list(info['pure_rewards'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f24d08",
   "metadata": {},
   "source": [
    "<h3> Battery First Policy: </h3>\n",
    "The action chosen is always 1, meaning that the battery is always used before interacting with the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1a8ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:36:41.837231Z",
     "start_time": "2025-05-15T14:35:20.587974Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:21<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "alg = 'battery_first'\n",
    "rewards[alg] = {}\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    obs, info = env.reset(options={'eval_profile': str(profile)})\n",
    "    done = False\n",
    "    rewards[alg][profile] = {}\n",
    "    rewards[alg][profile]['pure'] = []\n",
    "\n",
    "    while not done:\n",
    "        action = np.array([1])  # Use the battery as much as possible \n",
    "        obs, reward, terminated, truncated, info = env.step(action)  \n",
    "        done = terminated or truncated\n",
    "        rewards[alg][profile]['pure'].append(list(info['pure_rewards'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cf9a6",
   "metadata": {},
   "source": [
    "<h3> 50-50 Policy: </h3>\n",
    "The action chosen is always 0.5, meaning that the battery is never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670df112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:38:07.240124Z",
     "start_time": "2025-05-15T14:36:41.839296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:25<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "alg = '50-50'\n",
    "rewards[alg] = {}\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    obs, info = env.reset(options={'eval_profile': str(profile)})\n",
    "    done = False\n",
    "    rewards[alg][profile] = {}\n",
    "    rewards[alg][profile]['pure'] = []\n",
    "\n",
    "    while not done:\n",
    "        action = np.array([0.5])  # Use the battery at 50%\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  \n",
    "        done = terminated or truncated\n",
    "        rewards[alg][profile]['pure'].append(list(info['pure_rewards'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab00ac",
   "metadata": {},
   "source": [
    "### PPO agent\n",
    "Here we load the previously created model `PPO_trained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2961f1",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:38:07.275353Z",
     "start_time": "2025-05-15T14:38:07.244903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install stable-baselines3\n",
    "#!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1271c-39b9-4dc9-bfe0-1c19afdabaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"gym4real/microgrid-v0\", n_envs=1, env_kwargs={'settings':params})\n",
    "\n",
    "alg = 'ppo'\n",
    "rewards[alg] = {}\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=1)\n",
    "vec_env = model.get_env()\n",
    "model = PPO.load(\"examples/microgrid/trained_models/PPO_trained\")\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    vec_env.set_options({'eval_profile': str(profile)})\n",
    "    obs = vec_env.reset()\n",
    "\n",
    "    cumulated_reward = 0\n",
    "    rewards[alg][profile] = {}\n",
    "    rewards[alg][profile]['pure'] = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, r, dones, info = vec_env.step(action)\n",
    "        done = dones[0]\n",
    "        rewards[alg][profile]['pure'].append(list(info[0]['pure_rewards'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67caf49-eab4-4d49-ba46-df241972bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open('examples/microgrid/test_results.json', 'w') as f:\n",
    "#    json.dump(rewards, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e0a44",
   "metadata": {},
   "source": [
    "## Result analysis\n",
    "Here we compare the average cumulated reward across all the test profiles among the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69852d-93fb-4e60-8f6e-0d2fc9b2b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open('examples/microgrid/test_results.json', 'r') as f:\n",
    "#    rewards = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51413190-0a1a-45a3-aefe-403af3cc5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), tight_layout=True)\n",
    "\n",
    "for i, alg in enumerate(rewards.keys()):\n",
    "    cum_rewards = []\n",
    "    for profile in rewards[alg].keys():\n",
    "        cum_rewards.append(np.cumsum([rewards[alg][profile]['pure'][i][0] + rewards[alg][profile]['pure'][i][1] for i in range(len(rewards[alg][profile]['pure']))]))\n",
    "    \n",
    "    means = np.mean(cum_rewards, axis=0)\n",
    "    stds = np.std(cum_rewards, axis=0)\n",
    "    ci = 1.96 * stds/np.sqrt(len(rewards[alg].keys()))\n",
    "    \n",
    "    ax.plot(means, label=alg_labels[alg], color=alg_color[alg])        \n",
    "    ax.fill_between(range(len(means)), means + ci, means - ci, color=alg_color[alg], alpha=0.1)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf8626-a7fe-4699-9b69-acee15543ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_data = {}\n",
    "colors = []\n",
    "\n",
    "for alg in rewards.keys():\n",
    "    box_data[alg_labels[alg]] = []\n",
    "    \n",
    "    for profile in rewards[alg].keys():\n",
    "        total_trad = np.sum([rewards[alg][profile]['pure'][i][0] for i in range(len(rewards[alg][profile]['pure']))])\n",
    "        total_deg = np.sum([rewards[alg][profile]['pure'][i][1] for i in range(len(rewards[alg][profile]['pure']))])\n",
    "        box_data[alg_labels[alg]].append(total_trad + total_deg)\n",
    "\n",
    "    colors.append(alg_color[alg])\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 4), tight_layout=True)\n",
    "box_plot = sns.boxplot(box_data, gap=.1, palette=colors, width=.8)\n",
    "\n",
    "medians = [np.mean(values) for key, values in box_data.items()]\n",
    "vertical_offset = -130 # offset from median for display\n",
    "\n",
    "for xtick, alg in zip(box_plot.get_xticks(), rewards.keys()):\n",
    "    box_plot.text(xtick, \n",
    "                  vertical_offset, \n",
    "                  round(medians[xtick]), \n",
    "                  horizontalalignment='center',\n",
    "                  size='x-small', \n",
    "                  color=alg_color[alg], \n",
    "                  weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc45cf-df24-42c1-80f3-812123632ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
